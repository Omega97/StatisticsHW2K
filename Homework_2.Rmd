---
title: "Homework_2"
author: "Stromieri, Cusma Fait, Esposito, Lucas"
date: "2023-10-27"
output: html_document
---

```{r, echo=FALSE}
set.seed(0)
```

## FSDS - Chapter 3



### Ex 3.12

Simulate random sampling from a normal population distribution with several n values to
illustrate the law of large numbers.


**Solution**

We simulate a list of independent samples from a normal population.

```{r 3_12}
n <- 100
x <- rnorm(n)
y <- rep(0, n)

for (i in 1:n) {
  y[i] <- mean(x[1:i])
}

x_axis = 1:n
plot(x_axis, abs(y), type='l', ylim=c(-0.1, +1.1), 
     main='Absolute sample mean', xlab='n', ylab='abs mean')
lines(c(1, n), c(0, 0), col='red')
lines(x_axis, +x_axis^(-1/2), col='red', lty='dashed')

```

The solid black line shows how the absolute sample mean is converging to 0 as
the sample size increases, while the dashed red line is its expected behavior. 



### Ex 3.18

Sunshine City, which attracts primarily retired people, has 90,000 residents with a mean age
of 72 years and a standard deviation of 12 years. The age distribution is skewed to the left. A
random sample of 100 residents of Sunshine City has $y = 70$ and $s = 11$.

(a) Describe the center and spread of the (i) population distribution, (ii) sample data distribution. What shape does the sample data distribution probably have? Why?

(b) Find the center and spread of the sampling distribution of $Y$ for $n = 100$. What shape
does it have and what does it describe?

(c) Explain why it would not be unusual to sample a person of age 60 in Sunshine City, but
it would be highly unusual for the sample mean to be 60, for a random sample of 100
residents.

(d) Describe the sampling distribution of $Y$ : (i) for a random sample of size n = 1; (ii) if you
sample all 90,000 residents.


**Solution**

a) 

The population distribution of Sunshine City's residents has a mean of 72 years and a standard deviation of 12 years. This means the center of the population distribution is at 72 years, and the spread, which is measured by the standard deviation, is 12 years. Since it's mentioned that the age distribution is skewed to the left, we can infer that it has a negatively skewed or left-skewed shape. This means that there may be a longer tail to the left of the mean, and the majority of the residents may be clustered to the right of the mean.

The sample data distribution represents a random sample of 100 residents from Sunshine City. The sample has a sample mean  of 70 years and a sample standard deviation of 11 years. The center of the sample data distribution is given by the sample mean, which is 70 years, and the spread is given by the sample standard deviation, which is 11 years.

The shape of the sample data distribution is likely to resemble the shape of the population distribution. This is because, for a sufficiently large random sample (typically n > 30), the central limit theorem comes into play. In this case, with a sample size of 100, it is reasonable to assume that the sample data distribution will have a shape close to a normal distribution (bell-shaped curve).

Additionally, since the population distribution is skewed to the left, the sample data distribution may still have some evidence of left-skewness, but it will be less pronounced than the population distribution. The larger the sample size, the more it tends to approximate a normal distribution.

b) 

As stated above, when we consider the mean of a large enough(usually $n\ge30$) random sample the Central Limit Theorem comes into play. Therefore, we have that 
$$
Y = \frac{\sum_{i=1}^n Y_i}{n} \sim N(\mu,\frac{\sigma^2}{n}) |_{\mu=72,\sigma=12,n=100} = N(72,\frac{12^2}{100}) 
$$
This states that the mean of the observations is approximately normally distributed, it resembles the theoretical mean and its variance decreases as $n$ increases.

c)

Sampling a person of age 60 would not be very unusual since we are considering a population with a mean of $\mu=72$ and a standard deviation of $\sigma=12$. In fact, $60$ is only one single standard deviation far from the mean. 
Furthermore, we have to take into account the fact that the population has a left-skewed distribution, thus, we expect a long left-tailed density distribution. 
All these considerations strengthen the thesis that sampling a 60 years old person would not be so unusual.

However, sampling a random sample with mean $60$ is very far from reality. In fact, as stated above, due to the CLT, when we consider the mean of large samples, their variance would decrease as their size increases. Moreover, for the same reasons, even the left-skewness hypothesis can not be considered.

In fact, as stated that the R code bellow, the probability to pick a random sample with a mean less or equal to $60$ for a sample size $n=100$ is less than $7.7\times 10^{-24}$
```{r 3_18}
n <- 100
mean <- 72
sd <- 12 / sqrt(n) 
pnorm(60,mean,sd)

```
d)

Firstly, we are asked to consider a single sample. Obviously, since $n=1$ and $1$ is not large, we cannot apply the CLT, therefore, the sample mean $\frac{Y}{n}=\frac{Y_1}{1}=Y_1$ has the same distribution as the population.
Secondly, if we consider a sample as large ($n=90,000$) as the population, then the concept of sample has no meaning anymore, so we don't need to use statistical inference tools, we can directly compute our metrics from the data that we have.


### Ex 3.28

A survey is planned to estimate the population proportion $\pi$ supporting more government action to address global warming. For a simple random sample, if $\pi$ may be near $0.50$, how large should $n$ be so that the standard error of the sample proportion is $0.04$?

**Solution**

The problem suggests to start from an initial uncertainty of a binomial distribution with parameter $p=1/2$, which is $\sigma_0 = 1/2$ 

$$SE = \frac{\sigma_0}{\sqrt n} $$
$$n =  \frac{\sigma_0^2}{SE^2} = \frac{0.5^2}{0.04^2} = 156.25 $$

In order for the standard error of the sample proportion to be $SE=0.04$, the
sample size should be at least around 156.



## FSDS - Chapter 4



### Ex 4.2

For a sequence of observations of a binary random variable, you observe the geometric random variable (Section 2.2.2) outcome of the first success on observation number $y = 3$. Find and plot the likelihood function.


**Solution**

The geometric distribution is $f(y;p) = (1-p)^{1-y} \space p$, and the first outcome of our geometric random variable $X$ is $y=3$. We can use this information to calculate the log-likelihood function:

$$l(p|\bar y) = \sum_{i=1}^n{log(f(y_i;p))} = \sum_{i=1}^n{log((1-p)^{1-y_i} \space p)} $$
$$ = \sum_{i=1}^n{[(1-y_i) \space log(1-p)+log(p)])} $$
$$ = \sum_{i=1}^n{[(1-y_i) \space log(1-p)+log(p)])} = log(1-p) (n-n \space \hat y) + n \space log(p)$$
$$ = n[log(1-p) (\space \hat y - 1) + \space log(p)]$$
In our case, $n=1$ and $y=3$, therefore:

$$ l(p|y=3) = 1*[log(1-p)(3-1) + log(p)] = 2 \space log(1-p) + log(p) $$
We can now compute the likelihood:

$$L(p|y=3)=exp(l(p|y=3)) = (1-p)^2 p$$

```{r 4_2}
p <- seq(0, 1, length.out=101)
plot(p, (1-p)^2 * p, type='l', main='Likelihood', ylab='L(p)')
```



### Ex 4.4

For the ```Students``` data file (Exercise 1.2 in Chapter 1) and corresponding population, find the ML estimate of the population proportion believing in life after death. Construct a Wald $95%$ confidence interval, using its formula (4.8). Interpret.

**Solution**
We know from the theory that the MLE for $p$, the parameter of a Binomial distribution $Y = \sum_{i=1}^n y_i$ where $y_i$ follows a Bernoulli distribution with parameter $p$ is given by 
$$
\hat{\pi} = \frac{1}{n} \sum_{i=1}^n y_i
$$
First, we estimate the probability $\hat \pi$:
```{r 4_4_a}
students <- read.table("http://stat4ds.rwth-aachen.de/data/Students.dat",header=T)
n <- nrow(students)
n_believers <- nrow(students[students$life == 1,])
pi_hat <- n_believers / n
pi_hat
```
Then, we are able to compute the $95\%$ confidence interval for $\pi$ using the formula
$$
\hat\pi\pm z_{\frac{\alpha}{2}}\sqrt{\frac{\hat\pi(1-\hat\pi)}{n}}
$$
So, we can compute the confidence interval:
```{r 4_4_c}
se <- sqrt(pi_hat * (1-pi_hat) / n )
ci <- pi_hat + pnorm(0.5 / 2) * se * c(1,-1)
ci
```




### Ex 4.38

For independent observations $y_1, . . . , y_n$ having the geometric distribution (2.1):

(a) Find a sufficient statistic for $\pi$.

(b) Derive the ML estimator of $\pi$.

**Solution**


a) Let's suppose we are working with i.i.d. observations $y_1,y_2,\ldots,y_n$ following the geometric distribution $f(y)=(1-\pi)^{y-1} \pi$.

The likelihood function for $\pi$ is: 
$$ L(\pi) = \pi^{n}(1-\pi)^{{\sum_{i=1}^{n}y_i} - n}$$.

We know that a sufficient statistic $T(Y)$ exists when the likelihood function factors as 
$L(\theta) = g[T(y);\theta]h(y)$ where the first function $g$ involves $\theta$ and depends on the data through $T(y)$ and the second function $h$ is independent of $\theta$.

We can now obtain a sufficient statistic for $\pi$ by multiplying the likelihood function by one:  
$$ L(\pi) = \pi^{n}(1-\pi)^{({\sum_{i=1}^{n}y_i}) - n} \cdot 1$$
where $g[T(y);\theta] = \pi^{n}(1-\pi)^{({\sum_{i=1}^{n}y_i}) - n}$, $T(Y) = {\sum_{i=1}^{n}y_i}$ and $h(y) = 1$.

We can gather that $T(Y) = {\sum_{i=1}^{n}y_i}$ is a sufficient statistic for $\pi$.

b) To find the maximum likelihood estimator for $\pi$ let's first compute the log likelihood function: $$ l(\pi) = n \space ln(\pi)+(\sum_{i=1}^{n}y_i) \space ln(1-\pi)$$

Then we differentiate and equate to zero : $$\frac{\partial l(\pi)}{\partial \pi} = \frac{n}{\pi} - \frac{(\sum_{i=1}^{n}y_i)-n}{1-\pi}$$
$$ \frac{n}{\pi} - \frac{(\sum_{i=1}^{n}y_i)-n}{1-\pi} = 0 $$
$$ \frac{n}{\pi} = \frac{(\sum_{i=1}^{n}y_i)-n}{1-\pi} $$
$$ n (1-\pi) = \pi (\sum_{i=1}^{n}y_i -n) $$
$$ n = \pi (n+ \sum_{i=1}^{n}y_i -n)$$
$$ \hat \pi = \frac{n}{\sum_{i=1}^{n}y_i} = \frac{1}{\hat y}$$
We can conclude that $\frac{1}{\hat y}$ is the maximum likelihood estimator for $\pi$.


### Ex 4.44

Refer to the previous two exercises. Consider the selling prices (in thousands of dollars) in the Houses data file mentioned in Exercise 4.31.

(a) Fit the normal distribution to the data by finding the ML estimates of µ and σ for that
distribution.

(b) Fit the log-normal distribution to the data by finding the ML estimates of its parameters.

(c) Find and compare the ML estimates of the mean and standard deviation of selling price
for the two distributions.

(d) Superimpose the fitted normal and log-normal distributions on a histogram of the data.
Which distribution seems to be more appropriate for summarizing the selling prices?


**Solution**


```{r packages}
library(MASS)

```
Let's download the data-set
```{r datset}
Houses <- read.table("http://stat4ds.rwth-aachen.de/data/Houses.dat", header=TRUE)
```

a) Using the "fitdistr" function let's fit the normal distribution to the data by finding the MLE for its parameters.
```{r fit}
normal <- fitdistr(Houses$price, "normal")
```
These are the estimates for $\mu$ and $\sigma$
```{r estimate}
normal$estimate
```
b) We can do the same for the log-normal distribution
```{r log fit}
lognormal <- fitdistr(Houses$price, "lognormal")
```
These are the estimates for $\mu$ and $\sigma$
```{r log estimate}
lognormal$estimate
```
c) For the normal distribution we already found the mean and standard deviation in point a):
```{r}
normal$estimate
```
Using the parameter estimates from point b) we can compute the mean and standard deviation for the log normal:
$E(Y) = e^{\mu+ \frac{\sigma^2}{2}}$ , $sd(Y)= \sqrt{(e^{\sigma^2}-1)* E(Y)^2}$.
```{r}
mu <- lognormal$estimate[[1]]
sigma <- lognormal$estimate[[2]]
```

```{r}
log_mean <- exp(mu+sigma^2/2)
log_mean
```
```{r}
log_sd <- sqrt( (exp(sigma^2)-1) * log_mean^2)
log_sd
```
As we can see the means we computed for the two distributions are almost the same, whereas the variance of the normal distribution is higher than the variance of the log-normal distribution.

d) We superimposed the fitted normal and log-normal on a histogram of the data
```{r hist}
par(mfrow = c(1,2))
domain <- 0:900
hist(Houses$price, xlab = 'house prices', main= 'Normal distribution', freq = FALSE)
points(dnorm(domain ,normal$estimate[[1]], normal$estimate[[2]] ), col = 'red', type = 'l' )

hist(Houses$price, xlab = 'house prices', main= 'Lognormal distribution', freq = FALSE)
points(dlnorm(domain ,lognormal$estimate[[1]], lognormal$estimate[[2]] ), col = 'red', type = 'l' )
```

By looking at the graphs the log-normal distribution seems the most appropriate to summarize the selling prices.
The data exhibits a right skewness that is better described by the log-normal distribution.



### Ex 4.48

For a simple random sample of n subjects, explain why it is about 95% likely that the sample
proportion has error no more than $1/\sqrt n$ in estimating the population proportion. (Hint: To
show this “$1/\sqrt n$ rule,” find two standard errors when $\pi = 0.50$, and explain how this compares to two standard errors at other values of $\pi$.) Using this result, show that $n = 1/M^2$
is a safe sample size for estimating a proportion to within M with 95% confidence.

**Solution**

Let's compute the standard error for the sample proportion $SE(\pi) = \sqrt{\frac{\pi (1-\pi)}{n}}$ for $\pi = 0.5$.
$$ SE(\pi) = \sqrt{\frac{0.5 ^2}{n}} = \frac{0.5}{\sqrt{n}}$$
We know that for a sample proportion a value of $\pi = 0.5$ produces the maximum standard error, this is due to the fact that we have the most uncertainty about the population proportion.

Let's compute other two SE for different values of $\pi$: $\pi_1 = 0.1$ and $\pi_2 = 0.8$.
$SE(\pi_1) = \sqrt{\frac{0.1*0.9}{n}}\rightarrow \frac{0.28}{\sqrt{n}}$ and $SE(\pi_2) = \sqrt{\frac{0.2*0.8}{n}}\rightarrow \frac{0.36}{\sqrt{n}}$.
Both these standard errors are lower than the standard error for $\pi = 0.5$, as expected.

Assuming we have a large enough sample size, the distribution of the sample proportion becomes approximately normal due to the Central Limit Theorem.

For a normal distribution roughly 95% of the data falls within two standard deviations of the mean.
When $\pi = 0.5$ about 95% of the time, the sample proportion will be within 2 * 0.5/√n of the true population proportion, meaning that the error is no more than $\frac{1}{\sqrt{n}}$.
The same applies to all the other values of $\pi$, since the standard error will be different but always smaller than the SE with $\pi = 0.5$.

To show that $n = \frac{1}{M^2}$ is a safe sample size to within M with 
95% probability, let's consider that $M = 1.96 * SE(\pi) =1.96 \sqrt{\frac{\pi (1-\pi)}{n}}$.
Let's work with $\pi = 0.5$, the value that maximizes the SE, solving for n we obtain that $n = \frac{1.96^2 * 0.5 ^ 2}{M^2} = \frac{0.96}{M^2}$. Since $n = \frac{1}{M^2} > \frac{0.96}{M^2}$ that is the worst scenario case for the SE, we can safely say that $n = \frac{1}{M^2}$ is an acceptable sample size.



## FSDS - Chapter 5



### Ex 5.2

When a government does not have enough money to pay for the services that it provides, it can raise taxes or it can reduce services. When the Florida Poll asked a random sample of 1200 Floridians which they preferred, $52%$ (624 of the 1200) chose raise taxes and $48%$ chose reduce services. Let $\pi$ denote the population proportion of Floridians who would choose raising taxes.
Analyze whether this is a minority of the population ($\pi < 0.50$) or a majority ($\pi > 0.50$) by testing $H_0 ∶ \pi = 0.50$ against $H_a: \pi \ne 0.50$. Interpret the P-value. Is it appropriate to “accept $H_0$? Why or why not?


**Solution**

In order to test whether the observed proportion $\pi$ represents a minority or a majority of the population, we first set the null hypothesis

$$
H_0: \pi = 0.50
$$

and the alternative hypothesis

$$
H_1: \pi \neq 0.50
$$
We then calculate the test statistic:

$$
z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}
$$
```{r}
z = (0.52 - 0.50) / sqrt(0.5*(1 - 0.5) / 1200)
```

and use such test statistic to determine the $p$-value of the hypothesized proportion, i.e. the probability that the sample proportion deviates from the one computed under $H_0$ (namely assuming $\pi$ is the population proportion). The computed $p$-value is doubled due to the double-sided nature of the considered alternative hypothesis.

$$
p = 2 \cdot P(Z \geq z) = 2 \cdot (1 - P(Z \leq z))
$$

```{r}
p = 2 * pnorm(z, 0, 1, lower.tail=F)
p
```

The $p$-value is interpreted as the aforementioned probability. Although we have not set a target level of significance \alpha, it is safe to consider a relatively large $p$-value, like the one obtained or in general $p \geq 0.05$, as evidence for not rejecting the null hypothesis. We can conclude that the null hypothesis, stating the population proportion $\pi$ to have chosen "raising taxes" in the survey is $0.50$, is not to be rejected, therefore rejecting the alternative hypothesis $H_1$. There is therefore insufficient evidence to state whether such proportion represents a minority or a majority of the entire population.


### Ex 5.12

The example in Section 3.1.4 described an experiment to estimate the mean sales with a proposed menu for a new restaurant. In a revised experiment to compare two menus, on Tuesday of the opening week the owner gives customers menu A and on Wednesday she gives them menu B. The bills average \$22.30 for the 43 customers on Tuesday ($s = 6.88$) and \$25.91 for the 50 customers on Wednesday ($s = 8.01$). Under the strong assumption that her customers each night are comparable to a random sample from the conceptual population of potential customers, show how to compare the mean sales for the two menus based on (a) the P-value of a significance test, (b) a $95%$ confidence interval. Which is more informative, and why? (When used in an experiment to compare two treatments to determine which works better, a two-sample test is often called an A/B test.) 

**Solution**

In order to perform this A/B test and analyze which of the menus (A or B) maximizes the mean sales for the restaurant, we setup a two-sample $t$-test to compare the sample means of sales distributions over the two scenarios (when menu A was served and when menu B was served). The analyzed null hypothesis is
$$
H_0: \mu_1 > \mu_2 \implies \mu_1 - \mu_2 > 0
$$
and the test statistic to be considered is

$$
\hat t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}
$$

where $X_1$ and $X_2$ are the r.v.s corresponding to the two scenarios (and $s_1$, $s_2$, $n_1$ and $n_2$ are standard deviation and sample size of, respectively, $X_1$ and $X_2$).

```{r}
mA = 22.30
mB = 25.91
nA = 43
nB = 50
sA = 6.88
sB = 8.01

t = (mA - mB) / sqrt(sA**2/nA + sB**2/nB)
```

Calculating $(a)$ the $p$-value for such test statistic we get

```{r}
p = pt(t, df=nA+nB-2)
p
```

providing too weak evidence ($p < 0.05$) to reject an alternative hypothesis to $H_0$. Using $(b)$ a $95\%$ confidence interval, instead, we compute

$$
\bar{x_1} - \bar{x_2} \pm t_{\alpha/2} \cdot \sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}
$$

yielding the interval

$$
CI_{95\%}=[-6.67, -0.54]
$$
for the difference $\mu_1 - \mu_2$. The mean difference computed on our data is $\mu_1 - \mu_2 = 22.3 - 25.91 = -3.91 \in CI_{95\%}$.

The $p$-value is informative about the categorical answer to the initial question: analyzing its value we can conclude whether our initial hypothesis is confirmed or must be rejected. The confidence interval, on the other hand, gives a numerical idea of the interval of interest: as long as the observed statistic (in this case, the difference between the sample means) falls in the computed interval, it is reasonably ensured that $H_0$ holds.


### Ex 5.68

Explain why the confidence interval based on the Wald test of $H_0: \theta = \theta_0$ is symmetric around $\hat \theta$ (i.e., having center exactly equal to $\hat \theta$. This is not true for the confidence intervals based on the likelihood-ratio and score tests.) Explain why such symmetry can be problematic when $\theta$ and $\hat \theta$ are near a boundary, using the example of a population proportion that is very close to 0 or 1 and a sample proportion that may well equal 0 or 1.

**Solution**

In building a confidence interval based on the Wald test for a binomial distribution, the parameter of interest is estimated by approximating the error distribution with a normal distribution. Therefore, by definition of the estimate of the parameter of interest $p_0$, the resulting interval will be symmetric around $p_0$ itself (mean of the distribution). As stated in literature, the performance of Wald test-based confidence interval can be poor with certain choices of binomial parameters, and in general when the sample size is not big enough or when the sought after proportion approaches its boundaries (tends to 0 or 1). In such scenarios, the symmetry of the unbounded error distribution can generate incompatible intervals, i.e. exceeding the boundaries, as obtained in the following snippet.

```{r 5_68}
waldInterval <- function(x, n, conf.level = 0.95) {
 p <- x/n
 serr <- sqrt(p*((1-p)/n))
 z <- qnorm((1 - conf.level) / 2)
 z
 c(p + z * serr, p - z * serr)
}

p <- 0.99
trials <- 100
n <- 1000
x <- rbinom(n, trials, p)
coverage <- as.numeric()
for (j in 1 : n) {
  ci <- waldInterval(x=x[j], n=trials)
  coverage[j] <- (ci[1] < p) & (p < ci[2])
}
mean(coverage)
```
The mean coverage shows the accuracy of the Wald interval for the fixed proportion $p$, highlighting a degrading performance when approaching, in this example, the upper boundary $1$.